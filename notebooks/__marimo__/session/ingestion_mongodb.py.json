{
  "version": "1",
  "metadata": {
    "marimo_version": "0.20.2"
  },
  "cells": [
    {
      "id": "Hbol",
      "code_hash": "34e622ca9a0e93b6a0beb9186b4f0a8f",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": []
    },
    {
      "id": "MJUe",
      "code_hash": "6ef0826a5a2f6bd489909601ddc94730",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": []
    },
    {
      "id": "vblA",
      "code_hash": "ccd616602283b618595ea500d950d095",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": [
        {
          "type": "stream",
          "name": "stdout",
          "text": "R\u00e9ussi ! 48 employ\u00e9s import\u00e9s avec succ\u00e8s.\n",
          "mimetype": "text/plain"
        }
      ]
    },
    {
      "id": "bkHC",
      "code_hash": "f0d33f7eb1682c8ef6125dbe88da6a28",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": [
        {
          "type": "stream",
          "name": "stderr",
          "text": "\r[Stage 0:>                                                          (0 + 1) / 1]\r\r                                                                                \r",
          "mimetype": "text/plain"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "Succ\u00e8s ! Spark a trouv\u00e9 48 documents.\n",
          "mimetype": "text/plain"
        },
        {
          "type": "stream",
          "name": "stderr",
          "text": "\r[Stage 3:>                                                          (0 + 1) / 1]\r\r                                                                                \r",
          "mimetype": "text/plain"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "+--------------------+--------------------+----------+------+----------+-----+---------+\n|                 _id|             adresse|anciennete|   nom|    prenom|prime|      tel|\n+--------------------+--------------------+----------+------+----------+-----+---------+\n|511d0a9f181b16509...|{31000, 77, NULL,...|        10|Dupond|      Jean| NULL|     NULL|\n|511d0a9f181b16509...|{33000, 78, NULL,...|        27|  King|     David| NULL|     NULL|\n|511d0a9f181b16509...|{75000, 75, NULL,...|         7|Ossola|Christophe| NULL|     NULL|\n|511d0a9f181b16509...|{31000, 80, Gener...|         2|Monnin|    Gilles| NULL|     NULL|\n|511d0a9f181b16509...|{9500, 79, NULL, ...|         5| Priou|    Franck| NULL|547608352|\n+--------------------+--------------------+----------+------+----------+-----+---------+\nonly showing top 5 rows\n\n",
          "mimetype": "text/plain"
        }
      ]
    },
    {
      "id": "lEQa",
      "code_hash": "51064d899de53434c35c55c9f5c0977c",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": [
        {
          "type": "stream",
          "name": "stdout",
          "text": ":: loading settings :: url = jar:file:/usr/local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
          "mimetype": "text/plain"
        },
        {
          "type": "stream",
          "name": "stderr",
          "text": "Ivy Default Cache set to: /root/.ivy2/cache\nThe jars for the packages stored in: /root/.ivy2/jars\norg.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\norg.apache.hadoop#hadoop-aws added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent-6ced8213-3c22-44ba-af1f-0f8f9b6ed9fe;1.0\n\tconfs: [default]\n\tfound org.mongodb.spark#mongo-spark-connector_2.12;10.4.0 in central\n\tfound org.mongodb#mongodb-driver-sync;5.1.4 in central\n\t[5.1.4] org.mongodb#mongodb-driver-sync;[5.1.1,5.1.99)\n\tfound org.mongodb#bson;5.1.4 in central\n\tfound org.mongodb#mongodb-driver-core;5.1.4 in central\n\tfound org.mongodb#bson-record-codec;5.1.4 in central\n\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\ndownloading https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/10.4.0/mongo-spark-connector_2.12-10.4.0.jar ...\n\t[SUCCESSFUL ] org.mongodb.spark#mongo-spark-connector_2.12;10.4.0!mongo-spark-connector_2.12.jar (154ms)\ndownloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar ...\n\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.3.4!hadoop-aws.jar (253ms)\ndownloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/5.1.4/mongodb-driver-sync-5.1.4.jar ...\n\t[SUCCESSFUL ] org.mongodb#mongodb-driver-sync;5.1.4!mongodb-driver-sync.jar (79ms)\ndownloading https://repo1.maven.org/maven2/org/mongodb/bson/5.1.4/bson-5.1.4.jar ...\n\t[SUCCESSFUL ] org.mongodb#bson;5.1.4!bson.jar (1255ms)\ndownloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/5.1.4/mongodb-driver-core-5.1.4.jar ...\n\t[SUCCESSFUL ] org.mongodb#mongodb-driver-core;5.1.4!mongodb-driver-core.jar (4122ms)\ndownloading https://repo1.maven.org/maven2/org/mongodb/bson-record-codec/5.1.4/bson-record-codec-5.1.4.jar ...\n\t[SUCCESSFUL ] org.mongodb#bson-record-codec;5.1.4!bson-record-codec.jar (422ms)\ndownloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar ...\n\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.12.262!aws-java-sdk-bundle.jar (51407ms)\ndownloading https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.0.7.Final/wildfly-openssl-1.0.7.Final.jar ...\n\t[SUCCESSFUL ] org.wildfly.openssl#wildfly-openssl;1.0.7.Final!wildfly-openssl.jar (124ms)\n:: resolution report :: resolve 13630ms :: artifacts dl 57840ms\n\t:: modules in use:\n\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n\torg.mongodb#bson;5.1.4 from central in [default]\n\torg.mongodb#bson-record-codec;5.1.4 from central in [default]\n\torg.mongodb#mongodb-driver-core;5.1.4 from central in [default]\n\torg.mongodb#mongodb-driver-sync;5.1.4 from central in [default]\n\torg.mongodb.spark#mongo-spark-connector_2.12;10.4.0 from central in [default]\n\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   8   |   8   |   8   |   0   ||   8   |   8   |\n\t---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent-6ced8213-3c22-44ba-af1f-0f8f9b6ed9fe\n\tconfs: [default]\n\t8 artifacts copied, 0 already retrieved (277944kB/3028ms)\n26/02/28 12:01:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n26/02/28 12:02:24 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n26/02/28 12:02:25 ERROR Inbox: Ignoring error\njava.lang.NullPointerException: Cannot invoke \"org.apache.spark.storage.BlockManagerId.executorId()\" because \"idWithoutTopologyInfo\" is null\n\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:677)\n\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n26/02/28 12:02:25 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.storage.BlockManagerId.executorId()\" because \"idWithoutTopologyInfo\" is null\n\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:677)\n\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n\t... 3 more\n",
          "mimetype": "text/plain"
        }
      ]
    },
    {
      "id": "PKri",
      "code_hash": "82b310a4824cd23c54977750ec293249",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": [
        {
          "type": "stream",
          "name": "stdout",
          "text": "Envoi de 48 lignes vers MinIO...\n",
          "mimetype": "text/plain"
        },
        {
          "type": "stream",
          "name": "stderr",
          "text": "26/02/28 12:02:54 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n\r[Stage 10:>                                                         (0 + 1) / 1]\r\r                                                                                \r",
          "mimetype": "text/plain"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "\u2728 Ingestion termin\u00e9e : s3a://bronze/mongodb/employes/20260228_1202/\n",
          "mimetype": "text/plain"
        }
      ]
    },
    {
      "id": "Xref",
      "code_hash": "91203647f387cb967ef2ec8f3b559ec6",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": [
        {
          "type": "stream",
          "name": "stdout",
          "text": "\ud83d\udce4 Envoi vers MinIO...\n",
          "mimetype": "text/plain"
        },
        {
          "type": "stream",
          "name": "stderr",
          "text": "\r[Stage 14:>                                                         (0 + 1) / 1]\r\r                                                                                \r",
          "mimetype": "text/plain"
        },
        {
          "type": "stream",
          "name": "stdout",
          "text": "\u2728 Ingestion termin\u00e9e dans : s3a://bronze/mongodb/employes/donnees_finales/\n",
          "mimetype": "text/plain"
        }
      ]
    },
    {
      "id": "SFPL",
      "code_hash": "08428fb99152a354fb2a63a8ae208a2e",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": [
        {
          "type": "stream",
          "name": "stdout",
          "text": "\u2705 Lecture r\u00e9ussie ! Total : 48 employ\u00e9s.\n+--------------------+--------------------+----------+------+----------+-----+---------+\n|                 _id|             adresse|anciennete|   nom|    prenom|prime|      tel|\n+--------------------+--------------------+----------+------+----------+-----+---------+\n|511d0a9f181b16509...|{31000, 77, NULL,...|        10|Dupond|      Jean| NULL|     NULL|\n|511d0a9f181b16509...|{33000, 78, NULL,...|        27|  King|     David| NULL|     NULL|\n|511d0a9f181b16509...|{75000, 75, NULL,...|         7|Ossola|Christophe| NULL|     NULL|\n|511d0a9f181b16509...|{31000, 80, Gener...|         2|Monnin|    Gilles| NULL|     NULL|\n|511d0a9f181b16509...|{9500, 79, NULL, ...|         5| Priou|    Franck| NULL|547608352|\n+--------------------+--------------------+----------+------+----------+-----+---------+\nonly showing top 5 rows\n\n",
          "mimetype": "text/plain"
        }
      ]
    },
    {
      "id": "BYtC",
      "code_hash": null,
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": []
    }
  ]
}