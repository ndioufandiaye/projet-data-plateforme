{
  "version": "1",
  "metadata": {
    "marimo_version": "0.20.2"
  },
  "cells": [
    {
      "id": "Hbol",
      "code_hash": "ba1fedc0a1f2e6aee31e9797aa9cb4b8",
      "outputs": [
        {
          "type": "error",
          "ename": "exception",
          "evalue": "An error occurred while calling o135.parquet.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n\tat org.apache.spark.sql.execution.datasources.DataSource.makeQualified(DataSource.scala:128)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:471)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:559)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1SourceCommand(DataFrameWriter.scala:273)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveCommand(DataFrameWriter.scala:242)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:115)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:369)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2641)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2735)\n\t... 26 more\n",
          "traceback": []
        }
      ],
      "console": [
        {
          "type": "stream",
          "name": "stderr",
          "text": "WARNING: Using incubator modules: jdk.incubator.vector\n26/02/26 23:01:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nUsing Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n26/02/26 23:01:51 WARN DependencyUtils: Local jar /app/jars/postgresql-42.6.0.jar does not exist, skipping.\n26/02/26 23:01:52 INFO SparkContext: Running Spark version 4.1.1\n26/02/26 23:01:52 INFO SparkContext: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64\n26/02/26 23:01:52 INFO SparkContext: Java version 21.0.10+7-Debian-1deb13u1\n26/02/26 23:01:53 INFO ResourceUtils: ==============================================================\n26/02/26 23:01:53 INFO ResourceUtils: No custom resources configured for spark.driver.\n26/02/26 23:01:53 INFO ResourceUtils: ==============================================================\n26/02/26 23:01:53 INFO SparkContext: Submitted application: Ingestion Bronze\n26/02/26 23:01:53 INFO SecurityManager: Changing view acls to: root\n26/02/26 23:01:53 INFO SecurityManager: Changing modify acls to: root\n26/02/26 23:01:53 INFO SecurityManager: Changing view acls groups to: root\n26/02/26 23:01:53 INFO SecurityManager: Changing modify acls groups to: root\n26/02/26 23:01:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled\n26/02/26 23:01:58 INFO Utils: Successfully started service 'sparkDriver' on port 42901.\n26/02/26 23:01:59 INFO SparkEnv: Registering MapOutputTracker\n26/02/26 23:01:59 INFO SparkEnv: Registering BlockManagerMaster\n26/02/26 23:01:59 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n26/02/26 23:01:59 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n26/02/26 23:01:59 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n26/02/26 23:01:59 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0f0796c2-ee4a-4b2a-93e0-f2db005689f0\n26/02/26 23:01:59 INFO SparkEnv: Registering OutputCommitCoordinator\n26/02/26 23:02:00 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n26/02/26 23:02:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n26/02/26 23:02:00 ERROR SparkContext: Failed to add /app/jars/postgresql-42.6.0.jar to Spark environment\njava.io.FileNotFoundException: Jar /app/jars/postgresql-42.6.0.jar not found\n\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2185)\n\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2241)\n\tat org.apache.spark.SparkContext.$anonfun$new$16(SparkContext.scala:544)\n\tat org.apache.spark.SparkContext.$anonfun$new$16$adapted(SparkContext.scala:544)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:936)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:544)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n26/02/26 23:02:00 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n26/02/26 23:02:00 INFO ResourceProfile: Limiting resource is cpu\n26/02/26 23:02:00 INFO ResourceProfileManager: Added ResourceProfile id: 0\n26/02/26 23:02:00 INFO SecurityManager: Changing view acls to: root\n26/02/26 23:02:00 INFO SecurityManager: Changing modify acls to: root\n26/02/26 23:02:00 INFO SecurityManager: Changing view acls groups to: root\n26/02/26 23:02:00 INFO SecurityManager: Changing modify acls groups to: root\n26/02/26 23:02:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled\n26/02/26 23:02:01 INFO Executor: Starting executor ID driver on host 21594d54281f\n26/02/26 23:02:01 INFO Executor: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64\n26/02/26 23:02:01 INFO Executor: Java version 21.0.10+7-Debian-1deb13u1\n26/02/26 23:02:01 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n26/02/26 23:02:01 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@1b8e831c for default.\n26/02/26 23:02:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35509.\n26/02/26 23:02:01 INFO NettyBlockTransferService: Server created on 21594d54281f:35509\n26/02/26 23:02:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n26/02/26 23:02:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 21594d54281f, 35509, None)\n26/02/26 23:02:02 INFO BlockManagerMasterEndpoint: Registering block manager 21594d54281f:35509 with 434.4 MiB RAM, BlockManagerId(driver, 21594d54281f, 35509, None)\n26/02/26 23:02:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 21594d54281f, 35509, None)\n26/02/26 23:02:02 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 21594d54281f, 35509, None)\n",
          "mimetype": "text/plain"
        }
      ]
    }
  ]
}